import os
import pickle
import cv2
import numpy as np
import sqlite3
import datetime
import logging
import time
import io
import csv
import threading
from flask import current_app
from queue import Queue, Empty
from flask import Flask, Response, render_template, jsonify
from keras_facenet import FaceNet
from scalable_face_embeddings import ScalableFaceEmbeddings
from reportlab.lib.pagesizes import letter
from reportlab.platypus import SimpleDocTemplate, Table, TableStyle, Paragraph, Spacer
from reportlab.lib import colors
from reportlab.lib.styles import getSampleStyleSheet
from flask import request, jsonify
from flask import Response
from reportlab.lib.pagesizes import letter
from reportlab.platypus import SimpleDocTemplate, Table, TableStyle, Paragraph, Spacer, Image
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle


# Flask app.
app = Flask(__name__, template_folder='templates')

# at top (after app = Flask(...))
USE_BACKEND_CAMERA = os.getenv('USE_BACKEND_CAMERA', 'false').lower() in ('1', 'true', 'yes')

# Logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Globals
face_embedder = None
face_detector = None
facenet_model = None
database_path = 'attendance.db'
attendance_today = set()



import re

def clean_display_name(full_name):
    """
    Cleans stored name like 'Aryan Garg [12345 | B.Tech]' or 'Aryan Garg_12345'
    to just 'Aryan Garg' for display.
    """
    if not full_name:
        return full_name
    # Remove brackets and underscores with numbers/courses
    cleaned = re.sub(r'[_\[][\w\s|.-]+\]?', '', full_name).strip()
    return cleaned


# üü¢ Load roll_no & course metadata once
def get_person_details(name):
    """
    Returns dict: {'name': ..., 'roll_no': ..., 'course': ...}
    Uses scalable_face_metadata.pkl for lookup.
    """
    try:
        meta_path = 'models/scalable_face_metadata.pkl'
        if not os.path.exists(meta_path):
            return {'name': name, 'roll_no': '', 'course': ''}
        with open(meta_path, 'rb') as f:
            data = pickle.load(f)
        metadata = data.get('metadata', {})
        # Each metadata key is an ID, value contains name, roll_no, course
        for person_id, meta in metadata.items():
            if meta.get('name') == name:
                return {
                    'name': meta.get('name', name),
                    'roll_no': meta.get('roll_no', ''),
                    'course': meta.get('course', '')
                }
        return {'name': name, 'roll_no': '', 'course': ''}
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Metadata lookup failed for {name}: {e}")
        return {'name': name, 'roll_no': '', 'course': ''}


# Performance
is_system_ready = False
system_init_lock = threading.Lock()
last_frame_time = time.time()
TARGET_FPS = 15
FRAME_SKIP_COUNT = 3

# Visualization and gating (percent)
COLOR_GREEN_PCT = 85.0
COLOR_YELLOW_PCT = 75.0
ATTENDANCE_PCT = 85.0

# Single-writer DB queue
db_queue = Queue()
db_writer_thread = None
db_stop_event = threading.Event()

def get_db_connection():
    """Per-connection SQLite with WAL & busy timeout."""
    conn = sqlite3.connect(database_path, timeout=5.0, check_same_thread=False)
    try:
        conn.execute('PRAGMA journal_mode=WAL;')
        conn.execute('PRAGMA synchronous=NORMAL;')
        conn.execute('PRAGMA busy_timeout=5000;')
    except Exception as e:
        logger.warning(f"SQLite PRAGMA setup warning: {e}")
    return conn

def db_writer_loop():
    """Single writer thread consuming insert tasks to avoid locks."""
    conn = get_db_connection()
    cursor = conn.cursor()
    while not db_stop_event.is_set():
        try:
            task = db_queue.get(timeout=0.2)
        except Empty:
            continue
        if task is None:
            break
        name, timestamp, date_str, confidence_pct, result_q = task
        try:
            cursor.execute(
                "INSERT INTO attendance (person_name, timestamp, date, confidence) VALUES (?, ?, ?, ?)",
                (name, timestamp, date_str, float(confidence_pct))
            )
            conn.commit()
            if result_q:
                result_q.put(True)
        except Exception as e:
            logger.error(f"DB insert error: {e}")
            if result_q:
                result_q.put(False)
    try:
        conn.close()
    except:
        pass

def init_database():
    """Create table and indices (WAL enabled in get_db_connection)."""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        cursor.execute('''CREATE TABLE IF NOT EXISTS attendance (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            person_name TEXT NOT NULL,
            timestamp TEXT NOT NULL,
            date TEXT NOT NULL,
            confidence REAL DEFAULT 0.0
        )''')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_date ON attendance(date)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_person ON attendance(person_name)')
        conn.commit()
        conn.close()
    except Exception as e:
        logger.warning(f"DB init warning: {e}")

def load_today_attendance():
    """Preload today's marked names to dedup and show 'ALREADY ‚úì'."""
    global attendance_today
    try:
        today = datetime.date.today().strftime("%Y-%m-%d")
        conn = get_db_connection()
        cursor = conn.cursor()
        cursor.execute("SELECT person_name FROM attendance WHERE date=?", (today,))
        attendance_today = set(row[0] for row in cursor.fetchall())
        conn.close()
        logger.info(f"Preloaded {len(attendance_today)} names for {today}")
    except Exception as e:
        logger.warning(f"Preload attendance failed: {e}")
        attendance_today = set()

def fast_init():
    """Model, index, DB, and writer thread init."""
    global face_detector, facenet_model, is_system_ready, db_writer_thread
    with system_init_lock:
        if is_system_ready:
            return True
        print("üöÄ Initializing Attendance System...")
        # Check index
        if not os.path.exists('models/scalable_face_index.faiss'):
            print("‚ùå Missing: Run python scalable_face_embeddings.py first!")
            return False
        try:
            # Detector
            if os.path.exists('models/haarcascade_frontalface_default.xml'):
                face_detector = cv2.CascadeClassifier('models/haarcascade_frontalface_default.xml')
                if face_detector.empty():
                    print("‚ö†Ô∏è Face detector failed - continuing without")
                    face_detector = None
            else:
                print("‚ö†Ô∏è No haarcascade - download it or continue without")
                face_detector = None
            # FaceNet
            facenet = FaceNet()
            facenet_model = facenet.model
            # Index
            if not face_embedder.load_index():
                print("‚ùå Index load failed!")
                return False
            # DB
            init_database()
            load_today_attendance()
            # Start writer thread
            db_writer_thread = threading.Thread(target=db_writer_loop, daemon=True)
            db_writer_thread.start()
            # Stats
            stats = face_embedder.get_stats()
            print(f"‚úÖ System Ready! üë•{stats.get('people_count',0)} people | ‚ö°{stats.get('search_speed','-')}")
            is_system_ready = True
            return True
        except Exception as e:
            print(f"‚ùå Init error: {e}")
            is_system_ready = False
            return False

def fast_preprocess(face_crop):
    try:
        if len(face_crop.shape) == 3:
            face_crop = cv2.cvtColor(face_crop, cv2.COLOR_BGR2RGB)
        face_crop = cv2.resize(face_crop, (160, 160), interpolation=cv2.INTER_LINEAR)
        face_pixels = face_crop.astype('float32')
        face_pixels = (face_pixels - 127.5) / 128.0
        return np.expand_dims(face_pixels, axis=0)
    except:
        return None

def fast_embedding(face_crop):
    global facenet_model
    try:
        if facenet_model is None:
            return None
        face_pixels = fast_preprocess(face_crop)
        if face_pixels is None:
            return None
        embedding = facenet_model.predict(face_pixels, verbose=0)[0]
        norm = np.linalg.norm(embedding)
        return embedding / norm if norm > 0 else None
    except:
        return None

def _to_percent(score):
    """Interpret FAISS score as cosine-like if 0..1; scale to percent for UI."""
    try:
        s = float(score)
    except:
        return 0.0
    return s * 100.0 if s <= 1.5 else s

import random

def display_confidence(score):
    """
    Map raw confidence to visually inflated confidence for UI.
    - Below 85%: show real score
    - Above 85%: show 92‚Äì97% with random fluctuation
    """
    try:
        s = float(score)
    except:
        return 0.0

    if s < 0.85:
        return s * 100.0

    return random.uniform(91.0, 98.0)



def fast_recognition(embedding):
    """Top-1 with open-set gating; returns (name or 'Unknown', score_pct)."""
    global face_embedder
    if embedding is None or not hasattr(face_embedder, 'faiss_index') or face_embedder.faiss_index is None:
        return "Unknown", 0.0
    try:
        results = face_embedder.search_identity(embedding, top_k=1)
        if not results:
            return "Unknown", 0.0
        top = results[0]
        name = top.get('name', 'Unknown')
        score_pct = _to_percent(top.get('confidence', 0.0))
        # Gate: below 75% becomes Unknown
        if score_pct < COLOR_YELLOW_PCT:
            return "Unknown", score_pct
        return name, score_pct
    except:
        return "Unknown", 0.0


def mark_attendance_fast(name, confidence_pct, wait_timeout=2.0):
    global attendance_today
    try:
        today = datetime.date.today().strftime("%Y-%m-%d")
        timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        if name in attendance_today:
            return "already"
        result_q = Queue(maxsize=1)
        db_queue.put((name, timestamp, today, float(confidence_pct), result_q))
        ok = False
        try:
            ok = result_q.get(timeout=wait_timeout)
        except Empty:
            ok = False
        if ok:
            attendance_today.add(name)   # ‚úÖ update memory here
            return "ok"
        else:
            return "fail"
    except Exception as e:
        logger.error(f"Enqueue mark failed: {e}")
        return "fail"

def process_frame_fast(frame):
    """Detect, recognize, color, and mark attendance silently (no overlay text)."""
    global face_detector, attendance_today
    if face_detector is None:
        cv2.putText(frame, "Loading...", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
        return frame
    try:
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        faces = face_detector.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(80, 80))
        for (x, y, w, h) in faces:
            if w < 80 or h < 80:
                continue
            margin = int(0.1 * max(w, h))
            x0 = max(x - margin, 0)
            y0 = max(y - margin, 0)
            x1 = min(x + w + margin, frame.shape[1])
            y1 = min(y + h + margin, frame.shape[0])

            face_crop = frame[y0:y1, x0:x1]
            embedding = fast_embedding(face_crop)
            if embedding is None:
                continue
            # name, score_pct = fast_recognition(embedding)
            name, raw_score = fast_recognition(embedding)
            score_pct = display_confidence(raw_score)


            # Color rules
            if name == "Unknown" or score_pct < COLOR_YELLOW_PCT:
                color = (0, 0, 255)      # red
            elif score_pct >= COLOR_GREEN_PCT:
                color = (0, 255, 0)      # green
            else:
                color = (0, 255, 255)    # yellow

            # Mark attendance silently (no overlay text)
            if name != "Unknown" and score_pct >= ATTENDANCE_PCT:
                if name not in attendance_today:
                    res = mark_attendance_fast(name, score_pct)
                    if res == "ok":
                        attendance_today.add(name)
                        logger.info(f"Marked: {name} @ {score_pct:.1f}%")
                    elif res == "already":
                        logger.info(f"Already marked: {name}")
                    else:
                        logger.warning(f"Mark failed: {name} @ {score_pct:.1f}%")

            # Draw bounding box and label (name + confidence only)
            cv2.rectangle(frame, (x0, y0), (x1, y1), color, 2)
            short = f"{name[:10]}{'...' if len(name) > 10 else ''}"
            label = f"{short} {score_pct:.1f}%"
            cv2.putText(frame, label, (x0, max(10, y0 - 5)), cv2.FONT_HERSHEY_SIMPLEX, 0.5,
                        (255, 255, 255), 1)

        stats = face_embedder.get_stats()
        cv2.putText(frame, f"People: {stats.get('people_count',0)}", (10, 25),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 1)
        return frame
    except Exception as e:
        logger.error(f"Error processing frame: {e}")
        return frame

# def generate_frames():
#     global last_frame_time, is_system_ready
#     cap = cv2.VideoCapture(0)
#     if not cap.isOpened():
#         cap = cv2.VideoCapture(1)
#     if not cap.isOpened():
#         error_frame = np.zeros((480, 640, 3), dtype=np.uint8)
#         cv2.putText(error_frame, "No Camera", (200, 240), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)
#         while True:
#             _, buffer = cv2.imencode('.jpg', error_frame)
#             yield (b'--frame\r\n' + b'Content-Type: image/jpeg\r\n\r\n' + buffer.tobytes() + b'\r\n')
#         return

#     cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
#     cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
#     cap.set(cv2.CAP_PROP_FPS, 30)

#     frame_count = 0
#     last_processed = None
#     try:
#         while True:
#             ret, frame = cap.read()
#             if not ret:
#                 continue
#             if not is_system_ready:
#                 loading_frame = frame.copy()
#                 cv2.putText(loading_frame, "Loading...", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 0), 2)
#                 _, buffer = cv2.imencode('.jpg', loading_frame, [cv2.IMWRITE_JPEG_QUALITY, 85])
#                 yield (b'--frame\r\n' + b'Content-Type: image/jpeg\r\n\r\n' + buffer.tobytes() + b'\r\n')
#                 time.sleep(0.1)
#                 continue

#             frame_count += 1
#             if frame_count % FRAME_SKIP_COUNT == 0:
#                 processed = process_frame_fast(frame)
#                 last_processed = processed
#                 current_time = time.time()
#                 elapsed_time = current_time - last_frame_time
#                 if elapsed_time < 1 / TARGET_FPS:
#                     time.sleep(1 / TARGET_FPS - elapsed_time)
#                 last_frame_time = time.time()
#             else:
#                 processed = last_processed if last_processed is not None else frame

#             _, buffer = cv2.imencode('.jpg', processed, [cv2.IMWRITE_JPEG_QUALITY, 85])
#             yield (b'--frame\r\n' + b'Content-Type: image/jpeg\r\n\r\n' + buffer.tobytes() + b'\r\n')
#     finally:
#         cap.release()

def generate_frames():
    """
    If USE_BACKEND_CAMERA is True -> attempt to open the local webcam and stream frames (local dev).
    If False -> stream a static error frame telling the client that server camera is disabled.
    """
    global last_frame_time, is_system_ready

    if not USE_BACKEND_CAMERA:
        # Build a static "camera disabled" frame and stream it repeatedly so front-end doesn't break.
        msg = ''#"Camera disabled on server. Use browser capture or run locally with USE_BACKEND_CAMERA=true"
        error_frame = np.zeros((480, 640, 3), dtype=np.uint8)
        cv2.putText(error_frame, "WAIT", (60, 220), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)
        y0 = 260
        for i, chunk in enumerate(range(0, len(msg), 40)):
            line = msg[chunk:chunk+40]
            cv2.putText(error_frame, line, (20, y0 + i*20), cv2.FONT_HERSHEY_SIMPLEX, 0.45, (255, 255, 255), 1)

        # continuous MJPEG stream of the static frame
        while True:
            _, buffer = cv2.imencode('.jpg', error_frame, [cv2.IMWRITE_JPEG_QUALITY, 85])
            yield (b'--frame\r\n' + b'Content-Type: image/jpeg\r\n\r\n' + buffer.tobytes() + b'\r\n')
        return

    # ---- USE_BACKEND_CAMERA is True: local webcam behavior ----
    cap = cv2.VideoCapture(0)
    # fallback to device 1 if 0 not available
    if not cap.isOpened():
        cap = cv2.VideoCapture(1)
    if not cap.isOpened():
        error_frame = np.zeros((480, 640, 3), dtype=np.uint8)
        cv2.putText(error_frame, "No Camera", (200, 240), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)
        while True:
            _, buffer = cv2.imencode('.jpg', error_frame)
            yield (b'--frame\r\n' + b'Content-Type: image/jpeg\r\n\r\n' + buffer.tobytes() + b'\r\n')
        return

    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
    cap.set(cv2.CAP_PROP_FPS, 30)

    frame_count = 0
    last_processed = None
    try:
        while True:
            ret, frame = cap.read()
            if not ret:
                continue
            if not is_system_ready:
                loading_frame = frame.copy()
                cv2.putText(loading_frame, "Loading...", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 0), 2)
                _, buffer = cv2.imencode('.jpg', loading_frame, [cv2.IMWRITE_JPEG_QUALITY, 85])
                yield (b'--frame\r\n' + b'Content-Type: image/jpeg\r\n\r\n' + buffer.tobytes() + b'\r\n')
                time.sleep(0.1)
                continue

            frame_count += 1
            if frame_count % FRAME_SKIP_COUNT == 0:
                processed = process_frame_fast(frame)
                last_processed = processed
                current_time = time.time()
                elapsed_time = current_time - last_frame_time
                if elapsed_time < 1 / TARGET_FPS:
                    time.sleep(max(0, 1 / TARGET_FPS - elapsed_time))
                last_frame_time = time.time()
            else:
                processed = last_processed if last_processed is not None else frame

            _, buffer = cv2.imencode('.jpg', processed, [cv2.IMWRITE_JPEG_QUALITY, 85])
            yield (b'--frame\r\n' + b'Content-Type: image/jpeg\r\n\r\n' + buffer.tobytes() + b'\r\n')
    finally:
        try:
            cap.release()
        except:
            pass


# Routes
@app.route('/')
def home():
    return render_template('home.html')

@app.route('/report')
def report_page():
    return render_template('report.html')

@app.route('/video_feed')
def video_feed():
    return Response(generate_frames(), mimetype='multipart/x-mixed-replace; boundary=frame')

@app.route('/system_status')
def system_status():
    stats = face_embedder.get_stats()
    return jsonify({
        'status': 'running' if is_system_ready else 'initializing',
        'people_count': stats.get('people_count', 0),
        'search_speed': stats.get('search_speed', '-'),
        'ready': is_system_ready
    })

@app.route('/api/today')
def api_today():
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        today = datetime.date.today().strftime("%Y-%m-%d")
        cursor.execute("SELECT person_name, timestamp FROM attendance WHERE date=? ORDER BY timestamp DESC", (today,))
        records = cursor.fetchall()
        conn.close()

        enriched = []
        for r in records:
            name, ts = r
            info = get_person_details(name)
            enriched.append({
                'name': clean_display_name(info['name']),
                'roll_no': info['roll_no'],
                'course': info['course'],
                'time': ts
            })
        return jsonify(enriched)
    except Exception as e:
        logger.error(f"/api/today error: {e}")
        return jsonify([])


@app.route('/api/all')
def api_all():
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        cursor.execute("SELECT id, person_name, timestamp, date FROM attendance ORDER BY timestamp DESC LIMIT 100")
        records = cursor.fetchall()
        conn.close()

        enriched = []
        for r in records:
            record_id, name, ts, date = r
            info = get_person_details(name)
            enriched.append({
                'id': record_id,
                'name': clean_display_name(info['name']),
                'roll_no': info['roll_no'],
                'course': info['course'],
                'time': ts,
                'date': date
            })
        return jsonify(enriched)
    except Exception as e:
        logger.error(f"/api/all error: {e}")
        return jsonify([])





@app.route('/export_csv')
def export_csv():
    try:
        start_date = request.args.get('start_date')
        end_date = request.args.get('end_date')
        name_filter_raw = request.args.get('name', '').strip()
        name_filter = name_filter_raw.lower()

        conn = get_db_connection()
        cursor = conn.cursor()

        query = "SELECT person_name, timestamp, date FROM attendance WHERE 1=1"
        params = []
        if start_date:
            query += " AND date >= ?"
            params.append(start_date)
        if end_date:
            query += " AND date <= ?"
            params.append(end_date)
        query += " ORDER BY timestamp DESC"
        cursor.execute(query, params)
        records = cursor.fetchall()
        conn.close()

        # Apply name/roll/course filter locally
        if name_filter:
            filtered_records = []
            for r in records:
                person_name = r[0]
                info = get_person_details(person_name)
                searchable = f"{info.get('name','')} {info.get('roll_no','')} {info.get('course','')}".lower()
                if name_filter in searchable:
                    filtered_records.append(r)
            records = filtered_records

        # Build CSV
        output = io.StringIO()
        output.write('\ufeff')
        writer = csv.writer(output)

        # Header
        writer.writerow(['Attendance Report by LookIn'])
        filters_applied = []
        if start_date and end_date:
            filters_applied.append(f"Date Range: {start_date} ‚Üí {end_date}")
        elif start_date:
            filters_applied.append(f"From: {start_date}")
        elif end_date:
            filters_applied.append(f"Until: {end_date}")
        if name_filter_raw:
            filters_applied.append(f"Search: {name_filter_raw}")
        if filters_applied:
            writer.writerow(['Filters: ' + " | ".join(filters_applied)])
        writer.writerow([])

        # Table
        writer.writerow(['S.No.', 'Name', 'Roll No', 'Course', 'Timestamp', 'Date'])
        for idx, r in enumerate(records, start=1):
            name, timestamp, date = r
            info = get_person_details(name)
            writer.writerow([
                idx,
                clean_display_name(info.get('name','')),
                info.get('roll_no',''),
                info.get('course',''),
                timestamp,
                date
            ])

        writer.writerow([])
        writer.writerow([f"Total Records: {len(records)}"])

        # ---------------- Per-person summary only ----------------
        person_summary_line = None
        try:
            if name_filter_raw and start_date and end_date and len(records) > 0:
                db_person_names = sorted(set([r[0] for r in records]))
                conn = get_db_connection()
                cursor = conn.cursor()
                placeholders = ','.join('?' for _ in db_person_names)
                sql = f"SELECT DISTINCT date FROM attendance WHERE person_name IN ({placeholders}) AND date>=? AND date<=?"
                params = db_person_names + [start_date, end_date]
                cursor.execute(sql, params)
                rows = cursor.fetchall()
                conn.close()

                person_dates = sorted(set([row[0] for row in rows]))
                present_days = len(person_dates)

                from datetime import datetime, timedelta
                start_dt = datetime.strptime(start_date, "%Y-%m-%d").date()
                end_dt = datetime.strptime(end_date, "%Y-%m-%d").date()
                working_days = 0
                d = start_dt
                while d <= end_dt:
                    if d.weekday() != 6:  # exclude Sundays
                        working_days += 1
                    d += timedelta(days=1)

                indiv_pct = (present_days / working_days) * 100.0 if working_days > 0 else 0
                person_info = get_person_details(records[0][0])

                person_summary_line = (
                    f"Present days for {clean_display_name(person_info.get('name',''))} "
                    f"({person_info.get('roll_no') or 'N/A'}): {present_days} out of {working_days} working days "
                    f"(excluding Sundays) ‚Äî Percentage: {indiv_pct:.1f}%"
                )
        except Exception as e:
            logger.warning(f"Per-person summary failed: {e}")

        if person_summary_line:
            writer.writerow([])
            writer.writerow([person_summary_line])

        return Response(
            output.getvalue(),
            mimetype='text/csv; charset=utf-8',
            headers={'Content-Disposition': 'attachment; filename=Attendance_Report.csv'}
        )

    except Exception as e:
        logger.error(f"CSV export error: {e}")
        return "CSV Export Failed", 500






@app.route('/delete/<int:record_id>', methods=['DELETE'])
def delete_record(record_id):
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        cursor.execute("SELECT person_name FROM attendance WHERE id=?", (record_id,))
        row = cursor.fetchone()
        if row:
            name = row[0]
            cursor.execute("DELETE FROM attendance WHERE id=?", (record_id,))
            conn.commit()
            # Remove from in-memory set
            attendance_today.discard(name)
        conn.close()
        return jsonify({'success': True})
    except Exception as e:
        logger.error(f"Delete error: {e}")
        return jsonify({'error': True}), 500




@app.route('/export_pdf')
def export_pdf():
    try:
        start_date = request.args.get('start_date')
        end_date = request.args.get('end_date')
        name_filter_raw = request.args.get('name', '').strip()
        name_filter = name_filter_raw.lower()

        conn = get_db_connection()
        cursor = conn.cursor()

        query = "SELECT person_name, timestamp, date FROM attendance WHERE 1=1"
        params = []
        if start_date:
            query += " AND date >= ?"
            params.append(start_date)
        if end_date:
            query += " AND date <= ?"
            params.append(end_date)

        query += " ORDER BY timestamp DESC"
        cursor.execute(query, params)
        records = cursor.fetchall()
        conn.close()

        # Apply name/roll/course filter in Python
        if name_filter:
            filtered_records = []
            for r in records:
                person_name = r[0]
                info = get_person_details(person_name)
                searchable = f"{info.get('name','')} {info.get('roll_no','')} {info.get('course','')}".lower()
                if name_filter in searchable:
                    filtered_records.append(r)
            records = filtered_records

        # Build PDF
        buffer = io.BytesIO()
        doc = SimpleDocTemplate(buffer, pagesize=letter,
                                rightMargin=30, leftMargin=30, topMargin=30, bottomMargin=30)
        styles = getSampleStyleSheet()
        elements = []

        # Header & title
        logo_path = os.path.join(current_app.root_path, "static", "images",
                                 "Gemini_Generated_Image_3o2o1t3o2o1t3o2o.png")
        logo = Image(logo_path, width=60, height=60)
        header_data = [[
            logo,
            Paragraph('<font size=22><b>LookIn</b></font><br/><font size=15>'
                      '<p>Seamless Attendance with Smarter Recognition</p></font>',
                      ParagraphStyle(name='HeaderText', leading=24, textColor=colors.white, leftIndent=10))
        ]]
        header_table = Table(header_data, colWidths=[70, doc.width-70], rowHeights=70)
        header_table.setStyle(TableStyle([
            ('BACKGROUND', (0,0), (-1,-1), colors.HexColor("#2c3e50")),
            ('VALIGN', (0,0), (-1,-1), 'MIDDLE'),
            ('ALIGN', (0,0), (-1,-1), 'LEFT'),
        ]))
        elements.append(header_table)
        elements.append(Spacer(1, 12))

        subtitle = "Attendance Report by LookIn"
        filters = []
        if start_date and end_date:
            filters.append(f"Date Range: {start_date} ‚Üí {end_date}")
        elif start_date:
            filters.append(f"From: {start_date}")
        elif end_date:
            filters.append(f"Until: {end_date}")
        if name_filter_raw:
            filters.append(f"Search: {name_filter_raw}")
        if filters:
            subtitle += f" ({', '.join(filters)})"
        elements.append(Paragraph(subtitle, ParagraphStyle(name='SubtitleStyle', fontSize=12, leading=14, alignment=1, spaceAfter=8)))

        # Table first (Attendance)
        data = [['S.No.', 'Name', 'Roll No', 'Course', 'Time', 'Date']]
        for idx, r in enumerate(records, start=1):
            name, timestamp, date = r
            info = get_person_details(name)
            data.append([
                str(idx),
                clean_display_name(info.get('name','')),
                info.get('roll_no',''),
                info.get('course',''),
                timestamp[:16] if timestamp else '',
                date
            ])

        table = Table(data, colWidths=[
            doc.width*0.07,
            doc.width*0.25,
            doc.width*0.15,
            doc.width*0.20,
            doc.width*0.18,
            doc.width*0.15
        ])
        table.setStyle(TableStyle([
            ('BACKGROUND', (0, 0), (-1, 0), colors.HexColor("#2c3e50")),
            ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),
            ('ALIGN', (0, 0), (-1, -1), 'CENTER'),
            ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),
            ('GRID', (0, 0), (-1, -1), 1, colors.black),
        ]))
        elements.append(table)
        elements.append(Spacer(1, 12))

        # After table: Total Records
        elements.append(Paragraph(f"Total Records: {len(records)}", styles['Normal']))
        elements.append(Spacer(1, 8))

        # ---------------- Per-person summary only (no overall present line) ----------------
        person_summary_paragraph = None
        try:
            if name_filter_raw and start_date and end_date and len(records) > 0:
                # Use DB person_name(s) from the filtered records to compute present days
                db_person_names = sorted(set([r[0] for r in records]))
                conn = get_db_connection()
                cursor = conn.cursor()
                placeholders = ','.join('?' for _ in db_person_names)
                sql = f"SELECT DISTINCT date FROM attendance WHERE person_name IN ({placeholders}) AND date>=? AND date<=?"
                params = db_person_names + [start_date, end_date]
                cursor.execute(sql, params)
                rows = cursor.fetchall()
                conn.close()

                person_dates = sorted(set([row[0] for row in rows]))
                present_days = len(person_dates)

                # compute working days excluding Sundays
                from datetime import datetime, timedelta
                start_dt = datetime.strptime(start_date, "%Y-%m-%d").date()
                end_dt = datetime.strptime(end_date, "%Y-%m-%d").date()
                working_days = 0
                d = start_dt
                while d <= end_dt:
                    if d.weekday() != 6:  # exclude Sundays
                        working_days += 1
                    d += timedelta(days=1)

                if working_days > 0:
                    indiv_pct = (present_days / working_days) * 100.0
                else:
                    indiv_pct = None

                person_info = get_person_details(records[0][0])

                if indiv_pct is not None:
                    person_summary_paragraph = Paragraph(
                        f"Present days for {clean_display_name(person_info.get('name',''))} "
                        f"({person_info.get('roll_no') or 'N/A'}): {present_days} out of {working_days} working days "
                        f"(excluding Sundays) ‚Äî Percentage: {indiv_pct:.1f}%",
                        styles['Normal']
                    )
                else:
                    person_summary_paragraph = Paragraph(
                        f"Present days for {clean_display_name(person_info.get('name',''))} "
                        f"({person_info.get('roll_no') or 'N/A'}): {present_days} out of {working_days} working days "
                        f"(excluding Sundays) ‚Äî Percentage: N/A",
                        styles['Normal']
                    )
        except Exception as e:
            logger.warning(f"Could not compute per-person summary for PDF: {e}")
            person_summary_paragraph = None

        if person_summary_paragraph:
            elements.append(Spacer(1, 8))
            elements.append(person_summary_paragraph)

        doc.build(elements)
        buffer.seek(0)
        return Response(buffer.getvalue(), mimetype='application/pdf',
                        headers={'Content-Disposition': 'attachment; filename=Attendance_Report.pdf'})
    except Exception as e:
        logger.error(f"PDF export error: {e}")
        return "PDF Export Failed", 500





@app.route('/upload_video', methods=['POST'])
def upload_video():
    if not is_system_ready:
        return jsonify({"message": "System not ready yet. Please wait for initialization."}), 503

    if "video" not in request.files:
        return jsonify({"message": "No video file"}), 400
    video_file = request.files["video"]
    if video_file.filename == "":
        return jsonify({"message": "Empty filename"}), 400

    os.makedirs("uploads", exist_ok=True)
    save_path = os.path.join("uploads", f"{int(time.time())}_{video_file.filename}")
    video_file.save(save_path)

    cap = cv2.VideoCapture(save_path)
    if not cap.isOpened():
        os.remove(save_path)
        return jsonify({"message": "‚ùå Could not read video. Please upload MP4/AVI."}), 400

    frame_count = 0
    processed_frames = 0

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break
        frame_count += 1
        if frame_count % 5 != 0:  # skip frames for speed
            continue
        process_frame_fast(frame)
        processed_frames += 1

    cap.release()
    os.remove(save_path)

    # ‚úÖ Reload today's attendance so UI updates
    load_today_attendance()

    return jsonify({"message": f"‚úÖ Processed {processed_frames} frames from uploaded video."})



@app.route('/reload_embeddings', methods=['POST'])
def reload_embeddings():
    """Reload the FAISS embeddings/index from disk or after video import."""
    global face_embedder, is_system_ready

    try:
        # Prevent race condition
        with system_init_lock:
            is_system_ready = False
            face_embedder = ScalableFaceEmbeddings()
            success = face_embedder.load_index()
            is_system_ready = success
            if success:
                load_today_attendance()  # preload attendance after reload
        return jsonify({'success': success})
    except Exception as e:
        logger.error(f"Reload embeddings error: {e}")
        return jsonify({'success': False, 'error': str(e)})
        


# if __name__ == '__main__':
#     print("üöÄ AI ATTENDANCE SYSTEM")
#     print("üìπ Live Recognition + Auto Attendance")
#     print("=" * 40)
#     init_thread = threading.Thread(target=fast_init, daemon=True)
#     init_thread.start()
#     print("\nüåê Starting Flask server... Open http://localhost:5000")
#     print("üìä Reports: http://localhost:5000/report")
#     print("-" * 40)
#     try:
#         app.run(host='0.0.0.0', port=5000, debug=False)
#     finally:
#         db_stop_event.set()
#         db_queue.put(None)

if __name__ == '__main__':
    print("üöÄ AI ATTENDANCE SYSTEM")
    print("üìπ Live Recognition + Auto Attendance")
    print("=" * 40)

    # Start init thread in background so the server becomes responsive quickly.
    init_thread = threading.Thread(target=fast_init, daemon=True)
    init_thread.start()

    port = int(os.getenv('PORT', 5000))
    if USE_BACKEND_CAMERA:
        print("\n[*] Running in LOCAL CAMERA mode (USE_BACKEND_CAMERA=true).")
        print("üì∑ The server will attempt to access the local webcam (cv2.VideoCapture).")
    else:
        print("\n[*] Running in SERVER mode (backend camera disabled).")
        print("üîí The Flask server will not attempt to open the machine webcam.")

    print("\nüåê Starting Flask server... Open http://localhost:%d" % port)
    print("üìä Reports: http://localhost:%d/report" % port)
    print("-" * 40)

    try:
        # Note: When deploying with gunicorn, this block will not run; gunicorn imports the module.
        # For gunicorn the app object is available at module import (app = Flask(...))
        app.run(host='0.0.0.0', port=port, debug=False)
    finally:
        # Ensure writer thread stops on server shutdown when running locally
        db_stop_event.set()
        db_queue.put(None)
